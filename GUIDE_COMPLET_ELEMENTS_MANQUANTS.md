# üìä GUIDE COMPLET: √âl√©ments Manquants dans liverable_final.ipynb

## üéØ Vue d'Ensemble

Voici les **5 √©l√©ments majeurs** √† ajouter √† votre notebook final:

1. **Features Temporelles** (AvgWorkingHours, LateArrivals, etc.)
2. **SMOTE** (Gestion du d√©s√©quilibre des classes)
3. **Comparaison de Mod√®les** (Logistic Regression, Random Forest, SVM)
4. **Courbe ROC + AUC Score**
5. **Tableau R√©capitulatif Final**

---

## 1Ô∏è‚É£ FEATURES TEMPORELLES

### üìù Explication

Les fichiers `in_time.csv` et `out_time.csv` contiennent les heures d'arriv√©e/d√©part quotidiennes des employ√©s.  
On va cr√©er **5 nouvelles features** tr√®s pertinentes:

| Feature | Description | Pourquoi c'est important |
|---------|-------------|--------------------------|
| **AvgWorkingHours** | Moyenne des heures travaill√©es par jour | Mesure la charge de travail|
| **LateArrivals** | Nombre total de retards (arriv√©e apr√®s 9h) | Indicateur de d√©sengagement |
| **AvgOvertime** | Moyenne des heures suppl√©mentaires (> 8h) | **BURNOUT** üî• Principal pr√©dicteur d'attrition |
| **AbsenceRate** | Pourcentage de jours absents | Signe de d√©sint√©r√™t |
| **WorkHoursVariance** | Variance des heures de travail | Instabilit√©/horaires chaotiques |

### üí° Hypoth√®ses Business

- ‚ö†Ô∏è **Beaucoup d'heures sup** ‚Üí Burnout ‚Üí L'employ√© part
- ‚ö†Ô∏è **Nombreux retards** ‚Üí D√©sengagement ‚Üí L'employ√© part  
- ‚ö†Ô∏è **Variance √©lev√©e** ‚Üí Horaires instables ‚Üí L'employ√© part

### üìã Code √† Ajouter (Section 7.5 - Apr√®s le nettoyage)

```python
# =========================================================
# SECTION BONUS: Extraction Features Temporelles
# =========================================================

def extract_time_features():
    """
    Extrait 5 features depuis les fichiers in_time.csv et out_time.csv
    
    Returns:
        DataFrame avec EmployeeID + 5 nouvelles colonnes
    """
    print("‚è∞ Extraction des features temporelles...")
    
    # Charger les donn√©es
    in_time = pd.read_csv("data/in_time.csv")
    out_time = pd.read_csv("data/out_time.csv")
    
    employee_id_col = in_time.columns[0]
    date_columns = in_time.columns[1:]  # Toutes les dates
    
    features_list = []
    
    for idx, row in in_time.iterrows():
        employee_id = row[employee_id_col]
        
        # R√©cup√©rer les heures d'arriv√©e et de d√©part
        in_times = row[date_columns]
        out_times = out_time.iloc[idx][date_columns]
        
        # Convertir en datetime
        in_dt = pd.to_datetime(in_times, errors='coerce')
        out_dt = pd.to_datetime(out_times, errors='coerce')
        
        # Variables de calcul
        hours_worked = []
        late_count = 0
        overtime = []
        
        for in_t, out_t in zip(in_dt, out_dt):
            if pd.notna(in_t) and pd.notna(out_t):
                # Calcul heures travaill√©es
                hours = (out_t - in_t).total_seconds() / 3600
                hours_worked.append(hours)
                
                # Retards (apr√®s 9h00)
                if in_t.hour > 9 or (in_t.hour == 9 and in_t.minute > 0):
                    late_count += 1
                
                # Heures suppl√©mentaires (> 8h)
                if hours > 8:
                    overtime.append(hours - 8)
        
        # Cr√©er le dictionnaire de features
        features_list.append({
            'EmployeeID': employee_id,
            'AvgWorkingHours': round(np.mean(hours_worked) if hours_worked else 8.0, 2),
            'LateArrivals': late_count,
            'AvgOvertime': round(np.mean(overtime) if overtime else 0.0, 2),
            'AbsenceRate': round((len(date_columns) - len(hours_worked)) / len(date_columns) * 100, 2),
            'WorkHoursVariance': round(np.var(hours_worked) if len(hours_worked) > 1 else 0.0, 2)
        })
    
    df_time = pd.DataFrame(features_list)
    print(f"‚úÖ Features extraites pour {len(df_time)} employ√©s")
    return df_time

# EXECUTION
time_features = extract_time_features()

# Merge avec le DataFrame principal
print(f"\nAvant merge: {df.shape}")
df = df.merge(time_features, on="EmployeeID", how="left")
print(f"Apr√®s merge: {df.shape}")

# Afficher un aper√ßu
time_features.head()
```

### üìä Visualisation des Features Temporelles

```python
# S'assurer qu'Attrition est encod√© en num√©rique
if df['Attrition'].dtype == 'object':
    df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})

new_features = ['AvgWorkingHours', 'LateArrivals', 'AvgOvertime', 'AbsenceRate', 'WorkHoursVariance']

# Graphiques de distribution
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('üìä Distribution des Features Temporelles', fontsize=16, fontweight='bold')

for i, feat in enumerate(new_features):
    ax = axes[i//3, i%3]
    ax.hist(df[feat], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
    ax.axvline(df[feat].mean(), color='red', linestyle='--', 
               label=f"Moyenne: {df[feat].mean():.2f}")
    ax.set_title(feat)
    ax.set_xlabel(feat)
    ax.set_ylabel('Fr√©quence')
    ax.legend()
    ax.grid(alpha=0.3)

# Corr√©lation avec Attrition
ax = axes[1, 2]
corr = df[new_features + ['Attrition']].corr()['Attrition'].drop('Attrition').sort_values()
colors = ['red' if x < 0 else 'green' for x in corr.values]
ax.barh(corr.index, corr.values, color=colors, alpha=0.7)
ax.set_title('Corr√©lation avec Attrition', fontweight='bold')
ax.set_xlabel('Corr√©lation')
ax.axvline(0, color='black', linewidth=1)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Tableau comparatif Rest√©s vs Partis
print("\nüìä Analyse Comparative: Employ√©s Rest√©s vs Partis")
print("=" * 75)
print(f"{'Feature':<20} | {'Rest√©s (0)':<12} | {'Partis (1)':<12} | {'Diff√©rence':<12}")
print("-" * 75)

for feat in new_features:
    mean_stay = df[df['Attrition'] == 0][feat].mean()
    mean_left = df[df['Attrition'] == 1][feat].mean()
    diff = mean_left - mean_stay
    print(f"{feat:<20} | {mean_stay:>11.2f} | {mean_left:>11.2f} | {diff:>+11.2f}")

print("=" * 75)
```

### ‚úÖ R√©sultat Attendu

- **AvgOvertime** : Les employ√©s partis font significativement **plus d'heures sup**
- **LateArrivals** : Plus de retards chez ceux qui partent ‚Üí d√©sengagement
- **WorkHoursVariance** : Variance plus √©lev√©e ‚Üí instabilit√©

---

## 2Ô∏è‚É£ SMOTE (Gestion du D√©s√©quilibre)

### üìù Explication

Le dataset est **d√©s√©quilibr√©** : seulement ~16% d'attrition (Yes).  
SMOTE = **Synthetic Minority Over-sampling Technique**  
‚Üí Cr√©√© des exemples synth√©tiques de la classe minoritaire pour √©quilibrer

### üí° Pourquoi c'est Important

Sans SMOTE, le mod√®le va avoir tendance √† trop pr√©dire "No" (classe majoritaire).  
Avec SMOTE, on am√©liore la d√©tection des vrais cas d'attrition.

### üìã Code √† Ajouter (Juste avant l'entra√Ænement du mod√®le)

```python
# =========================================================
# SMOTE: Gestion du D√©s√©quilibre des Classes
# =========================================================

from imblearn.over_sampling import SMOTE

# S√©parer features (X) et target (y)
X = df.drop(['Attrition', 'EmployeeID'], axis=1, errors='ignore')
y = df['Attrition']

# Encodage des variables cat√©gorielles
le = LabelEncoder()
for col in X.select_dtypes(include='object').columns:
    X[col] = le.fit_transform(X[col].astype(str))

# Split Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# AVANT SMOTE
print("üìä Distribution AVANT SMOTE:")
print(f"  Classe 0 (No):  {(y_train == 0).sum()} ({(y_train == 0).sum() / len(y_train) * 100:.1f}%)")
print(f"  Classe 1 (Yes): {(y_train == 1).sum()} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)")

# Application de SMOTE
smote = SMOTE(sampling_strategy=0.7, random_state=42)  # 70% de la classe majoritaire
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# APR√àS SMOTE
print("\nüìä Distribution APR√àS SMOTE:")
print(f"  Classe 0 (No):  {(y_train_smote == 0).sum()} ({(y_train_smote == 0).sum() / len(y_train_smote) * 100:.1f}%)")
print(f"  Classe 1 (Yes): {(y_train_smote == 1).sum()} ({(y_train_smote == 1).sum() / len(y_train_smote) * 100:.1f}%)")

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].bar(['No', 'Yes'], [(y_train == 0).sum(), (y_train == 1).sum()], color=['green', 'red'])
axes[0].set_title('Avant SMOTE', fontweight='bold')
axes[0].set_ylabel('Nombre d\'employ√©s')

axes[1].bar(['No', 'Yes'], [(y_train_smote == 0).sum(), (y_train_smote == 1).sum()], color=['green', 'red'])
axes[1].set_title('Apr√®s SMOTE', fontweight='bold')
axes[1].set_ylabel('Nombre d\'employ√©s')

plt.tight_layout()
plt.show()
```

---

## 3Ô∏è‚É£ COMPARAISON DE MOD√àLES

### üìù Explication

On va tester **3 algorithmes** diff√©rents et comparer leurs performances:

1. **Logistic Regression** (Simple, interpr√©table)
2. **Random Forest** (D√©j√† dans le notebook, performances √©lev√©es)
3. **SVM** (Support Vector Machine, bon pour classification binaire)

### üìã Code √† Ajouter

```python
# =========================================================
# Comparaison de 3 Mod√®les de Classification
# =========================================================

from sklearn.metrics import roc_auc_score, roc_curve

# Standardisation (importante pour SVM et Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# Dictionnaire pour stocker les r√©sultats
results = {}

# -------------------- 1. LOGISTIC REGRESSION --------------------
print("üîπ Entra√Ænement Logistic Regression...")
lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
lr.fit(X_train_scaled, y_train_smote)

y_pred_lr = lr.predict(X_test_scaled)
y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]

results['Logistic Regression'] = {
    'model': lr,
    'y_pred': y_pred_lr,
    'y_proba': y_proba_lr,
    'accuracy': accuracy_score(y_test, y_pred_lr),
    'precision': precision_score(y_test, y_pred_lr),
    'recall': recall_score(y_test, y_pred_lr),
    'f1': f1_score(y_test, y_pred_lr),
    'auc': roc_auc_score(y_test, y_proba_lr)
}

# -------------------- 2. RANDOM FOREST --------------------
print("üîπ Entra√Ænement Random Forest...")
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train_smote, y_train_smote)

y_pred_rf = rf.predict(X_test)
y_proba_rf = rf.predict_proba(X_test)[:, 1]

results['Random Forest'] = {
    'model': rf,
    'y_pred': y_pred_rf,
    'y_proba': y_proba_rf,
    'accuracy': accuracy_score(y_test, y_pred_rf),
    'precision': precision_score(y_test, y_pred_rf),
    'recall': recall_score(y_test, y_pred_rf),
    'f1': f1_score(y_test, y_pred_rf),
    'auc': roc_auc_score(y_test, y_proba_rf)
}

# -------------------- 3. SVM --------------------
print("üîπ Entra√Ænement SVM...")
svm = SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')
svm.fit(X_train_scaled, y_train_smote)

y_pred_svm = svm.predict(X_test_scaled)
y_proba_svm = svm.predict_proba(X_test_scaled)[:, 1]

results['SVM'] = {
    'model': svm,
    'y_pred': y_pred_svm,
    'y_proba': y_proba_svm,
    'accuracy': accuracy_score(y_test, y_pred_svm),
    'precision': precision_score(y_test, y_pred_svm),
    'recall': recall_score(y_test, y_pred_svm),
    'f1': f1_score(y_test, y_pred_svm),
    'auc': roc_auc_score(y_test, y_proba_svm)
}

# -------------------- TABLEAU COMPARATIF --------------------
print("\n" + "="*80)
print("üìä TABLEAU COMPARATIF DES MOD√àLES")
print("="*80)
print(f"{'Mod√®le':<20} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10} | {'AUC':<10}")
print("-"*80)

for model_name, metrics in results.items():
    print(f"{model_name:<20} | {metrics['accuracy']:>9.3f} | {metrics['precision']:>9.3f} | "
          f"{metrics['recall']:>9.3f} | {metrics['f1']:>9.3f} | {metrics['auc']:>9.3f}")

print("="*80)

# Identifier le meilleur mod√®le (bas√© sur F1-Score)
best_model = max(results, key=lambda x: results[x]['f1'])
print(f"\nüèÜ MEILLEUR MOD√àLE: {best_model} (F1-Score = {results[best_model]['f1']:.3f})")
```

### üìä Graphique Comparatif

```python
# Visualisation comparative
metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']
model_names = list(results.keys())

fig, ax = plt.subplots(figsize=(14, 6))

x = np.arange(len(metrics_to_plot))
width = 0.25

for i, model_name in enumerate(model_names):
    values = [results[model_name][metric] for metric in metrics_to_plot]
    ax.bar(x + i*width, values, width, label=model_name, alpha=0.8)

ax.set_ylabel('Score', fontsize=12)
ax.set_title('Comparaison des Performances des Mod√®les', fontsize=14, fontweight='bold')
ax.set_xticks(x + width)
ax.set_xticklabels([m.capitalize() for m in metrics_to_plot])
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0, 1])

plt.tight_layout()
plt.show()
```

---

## 4Ô∏è‚É£ COURBE ROC + AUC SCORE

### üìù Explication

**ROC Curve** (Receiver Operating Characteristic):
- Montre le compromis entre **True Positive Rate** (Recall) et **False Positive Rate**
- Plus la courbe est proche du coin sup√©rieur gauche, meilleur est le mod√®le

**AUC** (Area Under the Curve):
- Score entre 0 et 1
- **AUC = 0.5** ‚Üí Mod√®le al√©atoire (comme lancer une pi√®ce)
- **AUC = 1.0** ‚Üí Mod√®le parfait
- **AUC > 0.8** ‚Üí Tr√®s bon mod√®le

### üìã Code √† Ajouter

```python
# =========================================================
# COURBE ROC + AUC SCORE
# =========================================================

fig, ax = plt.subplots(figsize=(10, 8))

colors = ['blue', 'green', 'red']

for (model_name, metrics), color in zip(results.items(), colors):
    # Calculer la courbe ROC
    fpr, tpr, thresholds = roc_curve(y_test, metrics['y_proba'])
    auc_score = metrics['auc']
    
    # Tracer la courbe
    ax.plot(fpr, tpr, color=color, lw=2, 
            label=f'{model_name} (AUC = {auc_score:.3f})')

# Ligne de r√©f√©rence (mod√®le al√©atoire)
ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Mod√®le Al√©atoire (AUC = 0.500)')

# Configuration du graphique
ax.set_xlabel('False Positive Rate (1 - Sp√©cificit√©)', fontsize=12)
ax.set_ylabel('True Positive Rate (Sensibilit√© / Recall)', fontsize=12)
ax.set_title('üìà Courbe ROC - Comparaison des Mod√®les', fontsize=14, fontweight='bold')
ax.legend(loc='lower right', fontsize=11)
ax.grid(alpha=0.3)
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])

# Ajouter la zone optimale
ax.fill_between([0, 0, 1], [0, 1, 1], alpha=0.1, color='green', 
                 label='Zone Optimale')

plt.tight_layout()
plt.show()

# Interpr√©tation
print("\nüìä INTERPR√âTATION DES R√âSULTATS:")
print("="*60)
for model_name, metrics in results.items():
    auc = metrics['auc']
    if auc >= 0.9:
        qualite = "üåü EXCELLENT"
    elif auc >= 0.8:
        qualite = "‚úÖ TR√àS BON"
    elif auc >= 0.7:
        qualite = "üëç BON"
    elif auc >= 0.6:
        qualite = "‚ö†Ô∏è MOYEN"
    else:
        qualite = "‚ùå FAIBLE"
    
    print(f"{model_name:<20} | AUC = {auc:.3f} | {qualite}")
print("="*60)
```

---

## 5Ô∏è‚É£ TABLEAU R√âCAPITULATIF FINAL

```python
# =========================================================
# SYNTH√àSE FINALE DU PROJET
# =========================================================

print("\n" + "="*80)
print(" "*25 + "üìä SYNTH√àSE DU PROJET" + " "*25)
print("="*80)

print("\n1Ô∏è‚É£ DONN√âES:")
print(f"   ‚Ä¢ Dataset fusionn√©: {df.shape[0]} employ√©s, {df.shape[1]} variables")
print(f"   ‚Ä¢ Taux d'attrition: {df['Attrition'].mean()*100:.1f}%")
print(f"   ‚Ä¢ Features temporelles ajout√©es: 5")

print("\n2Ô∏è‚É£ PR√âTRAITEMENT:")
print(f"   ‚Ä¢ Valeurs manquantes: TRAIT√âES (m√©diane/mode)")
print(f"   ‚Ä¢ SMOTE appliqu√©: OUI (ratio 70%)")
print(f"   ‚Ä¢ Standardisation: OUI (pour LR et SVM)")

print("\n3Ô∏è‚É£ MOD√àLES TEST√âS:")
best_auc = max([results[m]['auc'] for m in results])
for model_name in results:
    auc = results[model_name]['auc']
    marker = "üèÜ" if auc == best_auc else "  "
    print(f"   {marker} {model_name}: AUC = {auc:.3f}")

print("\n4Ô∏è‚É£ MEILLEUR MOD√àLE:")
print(f"   ‚Ä¢ Algorithme: {best_model}")
print(f"   ‚Ä¢ AUC Score: {results[best_model]['auc']:.3f}")
print(f"   ‚Ä¢ F1-Score: {results[best_model]['f1']:.3f}")
print(f"   ‚Ä¢ Recall: {results[best_model]['recall']:.3f}")

print("\n5Ô∏è‚É£ FEATURES LES PLUS IMPORTANTES (Random Forest):")
if 'Random Forest' in results:
    importances = results['Random Forest']['model'].feature_importances_
    feature_names = X.columns
    top_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:5]
    for i, (feat, imp) in enumerate(top_features, 1):
        print(f"   {i}. {feat:<25} {imp:.4f}")

print("\n6Ô∏è‚É£ RECOMMANDATIONS BUSINESS:")
print("   ‚úÖ Surveiller les employ√©s avec beaucoup d'heures suppl√©mentaires")
print("   ‚úÖ Am√©liorer l'√©quilibre vie pro/perso (WorkLifeBalance)")
print("   ‚úÖ Offrir des promotions r√©guli√®res (YearsSinceLastPromotion)")
print("   ‚úÖ R√©duire la distance domicile-travail (t√©l√©travail)")

print("\n" + "="*80)
```

---

## üìù INSTRUCTIONS D'INSTALLATION

Ajoutez ces cellules dans votre notebook dans cet ordre:

1. **Section 7.5** ‚Üí Features Temporelles (apr√®s nettoyage donn√©es)
2. **Section 8** ‚Üí SMOTE (juste avant entra√Ænement mod√®le)
3. **Section 9** ‚Üí Comparaison Mod√®les (remplacer l'entra√Ænement actuel)
4. **Section 10** ‚Üí Courbe ROC
5. **Section 11** ‚Üí Tableau R√©capitulatif

---

## ‚úÖ CHECKLIST FINALE

- [ ] Features temporelles extraites et visualis√©es
- [ ] SMOTE appliqu√© sur donn√©es d'entra√Ænement
- [ ] 3 mod√®les compar√©s (LR, RF, SVM)
- [ ] Courbe ROC trac√©e avec AUC pour chaque mod√®le
- [ ] Tableau r√©capitulatif ajout√©
- [ ] Explications d√©taill√©es en Markdown
- [ ] Mod√®le sauvegard√© avec joblib

---

üéâ **VOIL√Ä! Votre notebook est maintenant COMPLET et professionnel!**
