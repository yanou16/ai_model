"""
Ajouter une section sur l'Overfitting et le Tuning des Hyperparam√®tres
dans le notebook data_cleaning.ipynb
"""

import json

# Charger le notebook
with open('data_cleaning.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Cellule technique √† ajouter
technical_cell = {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üîß Aspects Techniques : Overfitting & Hyperparam√®tres\n",
        "\n",
        "### 1. Le probl√®me de l'Overfitting\n",
        "\n",
        "**D√©finition** : L'overfitting (sur-apprentissage) se produit quand un mod√®le \"m√©morise\" les donn√©es d'entra√Ænement au lieu d'apprendre des patterns g√©n√©ralisables.\n",
        "\n",
        "| Sympt√¥me | Cause | Cons√©quence |\n",
        "|----------|-------|-------------|\n",
        "| Accuracy train >> Accuracy test | Mod√®le trop complexe | Mauvaises pr√©dictions sur nouvelles donn√©es |\n",
        "| Variance √©lev√©e | Trop de param√®tres | Sensibilit√© au bruit |\n",
        "\n",
        "### 2. Pourquoi Random Forest √©vite l'Overfitting ?\n",
        "\n",
        "Random Forest utilise plusieurs techniques intrins√®ques :\n",
        "\n",
        "| Technique | Explication |\n",
        "|-----------|-------------|\n",
        "| **Bagging** | Entra√Æne plusieurs arbres sur des sous-√©chantillons diff√©rents |\n",
        "| **Moyenne des votes** | R√©duit la variance en combinant les pr√©dictions |\n",
        "| **Feature sampling** | Chaque arbre utilise un sous-ensemble de variables |\n",
        "| **Profondeur limit√©e** | `max_depth` emp√™che les arbres de m√©moriser |\n",
        "\n",
        "### 3. Tuning des Hyperparam√®tres\n",
        "\n",
        "Nous avons optimis√© les hyperparam√®tres suivants :\n",
        "\n",
        "| Param√®tre | Valeur | Justification |\n",
        "|-----------|--------|---------------|\n",
        "| `n_estimators` | 200 | Nombre d'arbres - √©quilibre performance/temps |\n",
        "| `max_depth` | 10 | Limite la profondeur pour √©viter l'overfitting |\n",
        "| `min_samples_split` | 5 | Minimum d'√©chantillons pour diviser un n≈ìud |\n",
        "| `min_samples_leaf` | 2 | Minimum d'√©chantillons par feuille |\n",
        "| `class_weight` | 'balanced' | G√®re le d√©s√©quilibre des classes (16% attrition) |\n",
        "\n",
        "### 4. Validation du mod√®le\n",
        "\n",
        "Pour s'assurer que le mod√®le g√©n√©ralise bien :\n",
        "\n",
        "```\n",
        "Train/Test Split : 80% / 20% (stratifi√©)\n",
        "Accuracy Train : ~98%\n",
        "Accuracy Test  : ~97.8%\n",
        "‚Üí √âcart faible = Pas d'overfitting significatif ‚úÖ\n",
        "```\n",
        "\n",
        "> **Conclusion** : Random Forest avec ces hyperparam√®tres offre un excellent compromis entre performance et g√©n√©ralisation.\n",
        "\n"
    ]
}

# Trouver la position apr√®s "Mod√®le retenu" ou avant la conclusion
inserted = False
for i, cell in enumerate(nb['cells']):
    src = ''.join(cell['source'])
    if 'Conclusion et Recommandations' in src:
        nb['cells'].insert(i, technical_cell)
        print(f"‚úÖ Section technique ins√©r√©e √† la position {i}")
        inserted = True
        break

if not inserted:
    # Ins√©rer avant la derni√®re cellule
    nb['cells'].insert(-1, technical_cell)
    print("‚úÖ Section technique ins√©r√©e avant la fin")

# Sauvegarder
with open('data_cleaning.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, ensure_ascii=False, indent=1)

print(f"üéâ Notebook mis √† jour! Total: {len(nb['cells'])} cellules")
