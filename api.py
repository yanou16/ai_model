"""
ðŸš€ API REST pour la PrÃ©diction d'Attrition des EmployÃ©s
Utilisation: python api.py
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import pickle
import os
import google.generativeai as genai
from groq import Groq
from dotenv import load_dotenv
import shap
import numpy as np

# Charger les variables d'environnement (.env)
load_dotenv()

# Configurer Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
    except Exception as e:
        print(f"âš ï¸ Erreur config Gemini: {e}")
else:
    print("âš ï¸ Attention: GEMINI_API_KEY non trouvÃ© dans les variables d'environnement.")

# Configurer Groq
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
groq_client = None
if GROQ_API_KEY:
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
    except Exception as e:
        print(f"âš ï¸ Erreur config Groq: {e}")
else:
    print("âš ï¸ Attention: GROQ_API_KEY non trouvÃ© dans les variables d'environnement.")

app = Flask(__name__)
CORS(app)  # Permet les requÃªtes depuis le frontend

# Charger le modÃ¨le au dÃ©marrage
MODEL_PATH = 'models/attrition_model.pkl'
model = None
explainer = None
preprocessor = None
classifier = None

def load_model():
    """Charge le modÃ¨le entraÃ®nÃ©"""
    global model, explainer, preprocessor, classifier
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ Erreur: Le modÃ¨le n'existe pas Ã  {MODEL_PATH}")
        print("ðŸ’¡ Lancez d'abord: python train_model.py")
        return False
    
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)

    # Initialiser SHAP Explainer
    try:
        print("ðŸ¤” Initialisation de SHAP Explainer...")
        # Check if model is a Pipeline
        if hasattr(model, 'named_steps'):
            preprocessor = model.named_steps['preprocessor']
            classifier = model.named_steps['classifier']
            explainer = shap.TreeExplainer(classifier)
            print(f"âœ… SHAP Explainer prÃªt (sur Pipeline: {type(classifier).__name__}).")
        else:
            # Fallback for raw model
            explainer = shap.TreeExplainer(model)
            print("âœ… SHAP Explainer prÃªt (sur ModÃ¨le brut).")
            
    except Exception as e:
        print(f"âš ï¸ Erreur init SHAP: {e}")
        explainer = None
    
    import time
    mod_time = os.path.getmtime(MODEL_PATH)
    time_str = time.ctime(mod_time)
    print(f"âœ… ModÃ¨le chargÃ© avec succÃ¨s!")
    print(f"ðŸ“… Timestamp du modÃ¨le: {time_str}")
    print(f"â˜¢ï¸  VERSION: NUCLEAR (GradientBoosting)")
    return True

@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint pour vÃ©rifier que l'API fonctionne"""
    return jsonify({
        'status': 'ok',
        'message': 'API de prÃ©diction d\'attrition opÃ©rationnelle',
        'model_loaded': model is not None
    })

@app.route('/fields', methods=['GET'])
def get_fields():
    """Retourne la liste des champs requis et leurs valeurs possibles"""
    return jsonify({
        'required_fields': {
            'Age': {'type': 'integer', 'description': 'Ã‚ge de l\'employÃ©'},
            'Gender': {'type': 'string', 'options': ['Male', 'Female']},
            'MaritalStatus': {'type': 'string', 'options': ['Single', 'Married', 'Divorced']},
            'DistanceFromHome': {'type': 'integer', 'description': 'Distance du domicile en km'},
            'Education': {'type': 'integer', 'min': 1, 'max': 5, 'description': '1=Below College, 2=College, 3=Bachelor, 4=Master, 5=Doctor'},
            'EducationField': {'type': 'string', 'options': ['Life Sciences', 'Medical', 'Marketing', 'Technical Degree', 'Other']},
            'Department': {'type': 'string', 'options': ['Sales', 'Research & Development', 'Human Resources']},
            'JobRole': {'type': 'string', 'description': 'Poste de l\'employÃ©'},
            'JobLevel': {'type': 'integer', 'min': 1, 'max': 5},
            'MonthlyIncome': {'type': 'integer', 'description': 'Revenu mensuel en $'},
            'TotalWorkingYears': {'type': 'integer', 'description': 'AnnÃ©es d\'expÃ©rience totales'},
            'YearsAtCompany': {'type': 'integer', 'description': 'AnnÃ©es dans l\'entreprise'},
            'YearsWithCurrManager': {'type': 'integer', 'description': 'AnnÃ©es avec le manager actuel'},
            'YearsSinceLastPromotion': {'type': 'integer', 'description': 'AnnÃ©es depuis derniÃ¨re promotion'},
            'NumCompaniesWorked': {'type': 'integer', 'description': 'Nombre d\'entreprises prÃ©cÃ©dentes'},
            'BusinessTravel': {'type': 'string', 'options': ['Non-Travel', 'Travel_Rarely', 'Travel_Frequently']},
            'PercentSalaryHike': {'type': 'integer', 'description': 'Augmentation salariale (%) derniÃ¨re annÃ©e'},
            'StockOptionLevel': {'type': 'integer', 'min': 0, 'max': 3},
            'TrainingTimesLastYear': {'type': 'integer', 'description': 'Formations suivies l\'annÃ©e derniÃ¨re'},
            'EnvironmentSatisfaction': {'type': 'integer', 'min': 1, 'max': 4},
            'JobSatisfaction': {'type': 'integer', 'min': 1, 'max': 4},
            'WorkLifeBalance': {'type': 'integer', 'min': 1, 'max': 4},
            'JobInvolvement': {'type': 'integer', 'min': 1, 'max': 4},
            'PerformanceRating': {'type': 'integer', 'min': 3, 'max': 4}
        },
        'optional_fields': {
            'AvgWorkingHours': {'type': 'float', 'description': 'Heures moyennes travaillÃ©es par jour', 'default': 8.5},
            'LateArrivals': {'type': 'integer', 'description': 'Nombre de retards (arrivÃ©es aprÃ¨s 9h)', 'default': 10},
            'AvgOvertime': {'type': 'float', 'description': 'Heures supplÃ©mentaires moyennes par jour', 'default': 0.5},
            'AbsenceRate': {'type': 'float', 'description': 'Taux d\'absence en %', 'default': 5.0},
            'WorkHoursVariance': {'type': 'float', 'description': 'Variance des heures de travail (rÃ©gularitÃ©)', 'default': 1.0}
        }
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Endpoint principal pour prÃ©dire l'attrition"""
    if model is None:
        return jsonify({
            'error': 'ModÃ¨le non chargÃ©',
            'message': 'Le modÃ¨le n\'a pas pu Ãªtre chargÃ© au dÃ©marrage'
        }), 500
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'DonnÃ©es manquantes', 'message': 'Veuillez fournir les donnÃ©es de l\'employÃ© en JSON'}), 400
        
        # Ajouter les features temporelles optionnelles par dÃ©faut si manquantes
        optional_time_features = {
            'AvgWorkingHours': 8.5, 'LateArrivals': 10, 'AvgOvertime': 0.5,
            'AbsenceRate': 5.0, 'WorkHoursVariance': 1.0
        }
        for feature, default_value in optional_time_features.items():
            if feature not in data:
                data[feature] = default_value
        
        employee_df = pd.DataFrame([data])
        
        prediction = model.predict(employee_df)[0]
        proba = model.predict_proba(employee_df)[0]
        
        proba_no = float(proba[0] * 100)
        proba_yes = float(proba[1] * 100)
        
        risk_level = 'low'
        if prediction == 1:
            if proba_yes > 70: risk_level = 'high'
            elif proba_yes > 50: risk_level = 'medium'
        
        recommendations = []
        if prediction == 1:
            recommendations = [
                "Organiser un entretien individuel",
                "Ã‰valuer les opportunitÃ©s de promotion",
                "AmÃ©liorer l'Ã©quilibre vie pro/perso",
                "Proposer des formations supplÃ©mentaires"
            ]
        else:
            recommendations = ["EmployÃ© satisfait - Continuer le bon travail!"]

        # 4. SHAP Explanation
        explanation_data = None
        if explainer and preprocessor is not None:
            try:
                # 1. Transform input data using the pipeline's preprocessor
                # SHAP needs the transformed numeric matrix that the model actually sees
                X_transformed = preprocessor.transform(employee_df)
                
                # 2. Variable names (need to retrieve feature names from OneHotEncoder)
                feature_names = []
                
                # Retrieve feature names from ColumnTransformer
                try:
                    feature_names = preprocessor.get_feature_names_out()
                except:
                    # Fallback
                    feature_names = [f"Feature {i}" for i in range(X_transformed.shape[1])]

                # 3. Calculate SHAP
                shap_valores = explainer.shap_values(X_transformed)
                
                # Handle binary classification
                vals = shap_valores
                if isinstance(shap_valores, list):
                    vals = shap_valores[1] # Class 1 = Yes
                elif len(np.array(shap_valores).shape) == 3: 
                    vals = np.array(shap_valores)[:,:,1]
                
                feature_values = vals[0] # Single sample
                
                # 4. Map back to JSON
                features_map = []
                for i, impact in enumerate(feature_values):
                    # Clean feature name
                    feat_name = str(feature_names[i])
                    features_map.append({
                        "feature": feat_name,
                        "impact": float(impact),
                        "value": "N/A"
                    })
                
                # Top Risk Factors
                risk_factors = sorted([f for f in features_map if f['impact'] > 0], key=lambda x: x['impact'], reverse=True)[:5]
                
                # Top Protective Factors
                protective_factors = sorted([f for f in features_map if f['impact'] < 0], key=lambda x: x['impact'])[:5]
                
                explanation_data = {
                    "risk_factors": risk_factors,
                    "protective_factors": protective_factors
                }
            except Exception as e:
                print(f"âš ï¸ SHAP Error during calculation: {e}")
                explanation_data = None
        
        response = {
            'prediction': {
                'will_leave': bool(prediction == 1),
                'label': 'Oui' if prediction == 1 else 'Non'
            },
            'probabilities': {
                'stay': round(proba_no, 2),
                'leave': round(proba_yes, 2)
            },
            'risk_level': risk_level,
            'recommendations': recommendations,
            'employee_data': data,
            'explainability': explanation_data
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        return jsonify({'error': 'Erreur lors de la prÃ©diction', 'message': str(e)}), 500

@app.route('/chat', methods=['POST'])
def chat():
    """Endpoint pour le chatbot (Gemini ou Groq)"""
    data = request.get_json()
    user_message = data.get('message', '')
    provider = data.get('provider', 'gemini') # 'gemini' or 'groq'
    
    if not user_message:
        return jsonify({'reply': "Je n'ai pas compris votre message."}), 400

    employee_context = ""
    employee_data = data.get('employee_data')
    prediction_result = data.get('prediction_result')

    if employee_data:
        employee_context += f"""
    CONTEXTE PROFIL EMPLOYÃ‰ (DonnÃ©es du formulaire) :
    {employee_data}
    """

    if prediction_result:
        probabilities = prediction_result.get('probabilities', {})
        risk_level = prediction_result.get('risk_level', 'inconnu')
        employee_context += f"""
    RÃ‰SULTAT DE LA PRÃ‰DICTION ACTUELLE :
    - Risque d'Attrition : {risk_level.upper()}
    - ProbabilitÃ© de DÃ©part : {probabilities.get('leave', 0)}%
    
    Utilise ces pourcentages pour justifier tes conseils.
    """

    context = f"""
    Tu es un Expert RH Analytique intÃ©grÃ© dans un Dashboard de PrÃ©diction d'Attrition.
    Ton rÃ´le est d'aider l'utilisateur Ã  comprendre pourquoi un employÃ© part, et Ã  simuler des scÃ©narios.

    DonnÃ©es du ModÃ¨le d'Attrition (Random Forest + SMOTE) :
    - Facteurs clÃ©s : TotalWorkingYears, Age, MonthlyIncome, YearsAtCompany, DistanceFromHome.

    {employee_context}

    Consignes :
    - Si une donnÃ©e contextuelle existe, utilise-la.
    - Sois concis, professionnel et direct.
    - RÃ©ponds en FranÃ§ais.
    """
    
    try:
        reply = ""
        
        if provider == 'groq':
            if not groq_client:
                 return jsonify({'reply': "âš ï¸ API Key Groq manquante. Configurez GROQ_API_KEY."}), 200
            
            print("ðŸš€ Utilisation de Groq (Llama 3)...")
            chat_completion = groq_client.chat.completions.create(
                messages=[{'role': 'system', 'content': context}, {'role': 'user', 'content': user_message}],
                model="llama-3.3-70b-versatile",
            )
            reply = chat_completion.choices[0].message.content
            
        else: # Default to Gemini
            if not GEMINI_API_KEY:
                 return jsonify({'reply': "âš ï¸ API Key Gemini manquante."}), 200
                 
            print("âœ¨ Utilisation de Gemini...")
            model = genai.GenerativeModel('gemini-robotics-er-1.5-preview')
            response = model.generate_content(f"{context}\n\nQuestion Utilisateur : {user_message}")
            reply = response.text

        return jsonify({'reply': reply})
        
    except Exception as e:
        error_msg = str(e)
        print(f"Erreur IA: {error_msg}")
        return jsonify({'reply': f"Erreur technique ({provider}) : {error_msg}"}), 200

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ðŸš€ API DE PRÃ‰DICTION D'ATTRITION DES EMPLOYÃ‰S")
    print("="*60 + "\n")
    
    if not load_model():
        print("\nâš ï¸  L'API va dÃ©marrer mais les prÃ©dictions ne fonctionneront pas.")
    
    print("\nðŸ“¡ Endpoints disponibles:")
    print("   GET  /health  - VÃ©rifier le statut de l'API")
    print("   POST /predict - PrÃ©dire l'attrition")
    print("   POST /chat    - Discuter avec l'Assistant RH")
    
    print("\nðŸŒ L'API dÃ©marre sur http://localhost:5000")
    print("="*60 + "\n")
    
    app.run(debug=True, host='0.0.0.0', port=5000)
