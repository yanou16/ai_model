"""
üöÄ API REST pour la Pr√©diction d'Attrition des Employ√©s
Utilisation: python api.py
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import pickle
import os
import google.generativeai as genai
from groq import Groq
from dotenv import load_dotenv
import shap
import numpy as np

# Charger les variables d'environnement (.env)
load_dotenv()

# Configurer Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur config Gemini: {e}")
else:
    print("‚ö†Ô∏è Attention: GEMINI_API_KEY non trouv√© dans les variables d'environnement.")

# Configurer Groq
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
groq_client = None
if GROQ_API_KEY:
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur config Groq: {e}")
else:
    print("‚ö†Ô∏è Attention: GROQ_API_KEY non trouv√© dans les variables d'environnement.")

app = Flask(__name__)
CORS(app)  # Permet les requ√™tes depuis le frontend

# Charger le mod√®le au d√©marrage
MODEL_PATH = 'models/attrition_model.pkl'
model = None
explainer = None
preprocessor = None
classifier = None

def load_model():
    """Charge le mod√®le entra√Æn√©"""
    global model, explainer, preprocessor, classifier
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Erreur: Le mod√®le n'existe pas √† {MODEL_PATH}")
        print("üí° Lancez d'abord: python train_model.py")
        return False
    
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)

    # Initialiser SHAP Explainer
    try:
        print("ü§î Initialisation de SHAP Explainer...")
        # Check if model is a Pipeline
        if hasattr(model, 'named_steps'):
            preprocessor = model.named_steps['preprocessor']
            classifier = model.named_steps['classifier']
            explainer = shap.TreeExplainer(classifier)
            print(f"‚úÖ SHAP Explainer pr√™t (sur Pipeline: {type(classifier).__name__}).")
        else:
            # Fallback for raw model
            explainer = shap.TreeExplainer(model)
            print("‚úÖ SHAP Explainer pr√™t (sur Mod√®le brut).")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur init SHAP: {e}")
        explainer = None
    
    import time
    mod_time = os.path.getmtime(MODEL_PATH)
    time_str = time.ctime(mod_time)
    print(f"‚úÖ Mod√®le charg√© avec succ√®s!")
    print(f"üìÖ Timestamp du mod√®le: {time_str}")
    print(f"üå≤ VERSION: Random Forest (anti-overfitting)")
    return True

@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint pour v√©rifier que l'API fonctionne"""
    return jsonify({
        'status': 'ok',
        'message': 'API de pr√©diction d\'attrition op√©rationnelle',
        'model_loaded': model is not None
    })

@app.route('/fields', methods=['GET'])
def get_fields():
    """Retourne la liste des champs requis et leurs valeurs possibles"""
    return jsonify({
        'required_fields': {
            'Age': {'type': 'integer', 'description': '√Çge de l\'employ√©'},
            'Gender': {'type': 'string', 'options': ['Male', 'Female']},
            'MaritalStatus': {'type': 'string', 'options': ['Single', 'Married', 'Divorced']},
            'DistanceFromHome': {'type': 'integer', 'description': 'Distance du domicile en km'},
            'Education': {'type': 'integer', 'min': 1, 'max': 5, 'description': '1=Below College, 2=College, 3=Bachelor, 4=Master, 5=Doctor'},
            'EducationField': {'type': 'string', 'options': ['Life Sciences', 'Medical', 'Marketing', 'Technical Degree', 'Other']},
            'Department': {'type': 'string', 'options': ['Sales', 'Research & Development', 'Human Resources']},
            'JobRole': {'type': 'string', 'description': 'Poste de l\'employ√©'},
            'JobLevel': {'type': 'integer', 'min': 1, 'max': 5},
            'MonthlyIncome': {'type': 'integer', 'description': 'Revenu mensuel en $'},
            'TotalWorkingYears': {'type': 'integer', 'description': 'Ann√©es d\'exp√©rience totales'},
            'YearsAtCompany': {'type': 'integer', 'description': 'Ann√©es dans l\'entreprise'},
            'YearsWithCurrManager': {'type': 'integer', 'description': 'Ann√©es avec le manager actuel'},
            'YearsSinceLastPromotion': {'type': 'integer', 'description': 'Ann√©es depuis derni√®re promotion'},
            'NumCompaniesWorked': {'type': 'integer', 'description': 'Nombre d\'entreprises pr√©c√©dentes'},
            'BusinessTravel': {'type': 'string', 'options': ['Non-Travel', 'Travel_Rarely', 'Travel_Frequently']},
            'PercentSalaryHike': {'type': 'integer', 'description': 'Augmentation salariale (%) derni√®re ann√©e'},
            'StockOptionLevel': {'type': 'integer', 'min': 0, 'max': 3},
            'TrainingTimesLastYear': {'type': 'integer', 'description': 'Formations suivies l\'ann√©e derni√®re'},
            'EnvironmentSatisfaction': {'type': 'integer', 'min': 1, 'max': 4},
            'JobSatisfaction': {'type': 'integer', 'min': 1, 'max': 4},
            'WorkLifeBalance': {'type': 'integer', 'min': 1, 'max': 4},
            'JobInvolvement': {'type': 'integer', 'min': 1, 'max': 4},
            'PerformanceRating': {'type': 'integer', 'min': 3, 'max': 4}
        },
        'optional_fields': {
            'AvgWorkingHours': {'type': 'float', 'description': 'Heures moyennes travaill√©es par jour', 'default': 8.5},
            'LateArrivals': {'type': 'integer', 'description': 'Nombre de retards (arriv√©es apr√®s 9h)', 'default': 10},
            'AvgOvertime': {'type': 'float', 'description': 'Heures suppl√©mentaires moyennes par jour', 'default': 0.5},
            'AbsenceRate': {'type': 'float', 'description': 'Taux d\'absence en %', 'default': 5.0},
            'WorkHoursVariance': {'type': 'float', 'description': 'Variance des heures de travail (r√©gularit√©)', 'default': 1.0}
        }
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Endpoint principal pour pr√©dire l'attrition"""
    if model is None:
        return jsonify({
            'error': 'Mod√®le non charg√©',
            'message': 'Le mod√®le n\'a pas pu √™tre charg√© au d√©marrage'
        }), 500
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Donn√©es manquantes', 'message': 'Veuillez fournir les donn√©es de l\'employ√© en JSON'}), 400
        
        # Ajouter les features temporelles optionnelles par d√©faut si manquantes
        optional_time_features = {
            'AvgWorkingHours': 8.5, 'LateArrivals': 10, 'AvgOvertime': 0.5,
            'AbsenceRate': 5.0, 'WorkHoursVariance': 1.0
        }
        for feature, default_value in optional_time_features.items():
            if feature not in data:
                data[feature] = default_value
        
        employee_df = pd.DataFrame([data])
        
        prediction = model.predict(employee_df)[0]
        proba = model.predict_proba(employee_df)[0]
        
        proba_no = float(proba[0] * 100)
        proba_yes = float(proba[1] * 100)
        
        # Seuils adapt√©s pour Random Forest (probabilit√©s compress√©es)
        # > 40% = high, 25-40% = medium, < 25% = low
        if proba_yes > 40:
            risk_level = 'high'
        elif proba_yes > 25:
            risk_level = 'medium'
        else:
            risk_level = 'low'
        
        # Recommandations bas√©es sur le niveau de risque
        if risk_level == 'high':
            recommendations = [
                "‚ö†Ô∏è URGENT: Organiser un entretien individuel imm√©diat",
                "√âvaluer les opportunit√©s de promotion ou changement de poste",
                "Revoir la charge de travail et les heures suppl√©mentaires",
                "Proposer un ajustement salarial si justifi√©"
            ]
        elif risk_level == 'medium':
            recommendations = [
                "Planifier un point r√©gulier avec le manager",
                "Am√©liorer l'√©quilibre vie pro/perso",
                "Proposer des formations de d√©veloppement",
                "√âvaluer la satisfaction environnementale"
            ]
        else:
            recommendations = [
                "Employ√© √† faible risque - Maintenir les bonnes conditions",
                "Continuer le suivi r√©gulier"
            ]

        # 4. SHAP Explanation
        explanation_data = None
        if explainer and preprocessor is not None:
            try:
                # 1. Transform input data using the pipeline's preprocessor
                # SHAP needs the transformed numeric matrix that the model actually sees
                X_transformed = preprocessor.transform(employee_df)
                
                # 2. Variable names (need to retrieve feature names from OneHotEncoder)
                feature_names = []
                
                # Retrieve feature names from ColumnTransformer
                try:
                    feature_names = preprocessor.get_feature_names_out()
                except:
                    # Fallback
                    feature_names = [f"Feature {i}" for i in range(X_transformed.shape[1])]

                # 3. Calculate SHAP
                shap_valores = explainer.shap_values(X_transformed)
                
                # Handle binary classification
                vals = shap_valores
                if isinstance(shap_valores, list):
                    vals = shap_valores[1] # Class 1 = Yes
                elif len(np.array(shap_valores).shape) == 3: 
                    vals = np.array(shap_valores)[:,:,1]
                
                feature_values = vals[0] # Single sample
                
                # 4. Map back to JSON
                features_map = []
                for i, impact in enumerate(feature_values):
                    # Clean feature name
                    feat_name = str(feature_names[i])
                    features_map.append({
                        "feature": feat_name,
                        "impact": float(impact),
                        "value": "N/A"
                    })
                
                # Top Risk Factors
                risk_factors = sorted([f for f in features_map if f['impact'] > 0], key=lambda x: x['impact'], reverse=True)[:5]
                
                # Top Protective Factors
                protective_factors = sorted([f for f in features_map if f['impact'] < 0], key=lambda x: x['impact'])[:5]
                
                explanation_data = {
                    "risk_factors": risk_factors,
                    "protective_factors": protective_factors
                }
            except Exception as e:
                print(f"‚ö†Ô∏è SHAP Error during calculation: {e}")
                explanation_data = None
        
        response = {
            'prediction': {
                'will_leave': bool(prediction == 1),
                'label': 'Oui' if prediction == 1 else 'Non'
            },
            'probabilities': {
                'stay': round(proba_no, 2),
                'leave': round(proba_yes, 2)
            },
            'risk_level': risk_level,
            'recommendations': recommendations,
            'employee_data': data,
            'explainability': explanation_data
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        return jsonify({'error': 'Erreur lors de la pr√©diction', 'message': str(e)}), 500

@app.route('/chat', methods=['POST'])
def chat():
    """Endpoint pour le chatbot (Gemini ou Groq)"""
    data = request.get_json()
    user_message = data.get('message', '')
    provider = data.get('provider', 'gemini') # 'gemini' or 'groq'
    
    if not user_message:
        return jsonify({'reply': "Je n'ai pas compris votre message."}), 400

    employee_context = ""
    employee_data = data.get('employee_data')
    prediction_result = data.get('prediction_result')

    if employee_data:
        employee_context += f"""
    CONTEXTE PROFIL EMPLOY√â (Donn√©es du formulaire) :
    {employee_data}
    """

    if prediction_result:
        # Handle both dict and float formats
        if isinstance(prediction_result, dict):
            probabilities = prediction_result.get('probabilities', {})
            risk_level = prediction_result.get('risk_level', 'inconnu')
            leave_prob = probabilities.get('leave', 0)
        else:
            # prediction_result is just the leave probability as a float
            leave_prob = prediction_result
            risk_level = '√©lev√©' if leave_prob > 50 else ('moyen' if leave_prob > 30 else 'faible')
        
        employee_context += f"""
    R√âSULTAT DE LA PR√âDICTION ACTUELLE :
    - Risque d'Attrition : {risk_level.upper()}
    - Probabilit√© de D√©part : {leave_prob}%
    
    Utilise ces pourcentages pour justifier tes conseils.
    """

    context = f"""
    Tu es un Expert RH Analytique int√©gr√© dans un Dashboard de Pr√©diction d'Attrition.
    Ton r√¥le est d'aider l'utilisateur √† comprendre pourquoi un employ√© part, et √† simuler des sc√©narios.

    Donn√©es du Mod√®le d'Attrition (Random Forest + SMOTE) :
    - Facteurs cl√©s : TotalWorkingYears, Age, MonthlyIncome, YearsAtCompany, DistanceFromHome.

    {employee_context}

    Consignes :
    - Si une donn√©e contextuelle existe, utilise-la.
    - Sois concis, professionnel et direct.
    - R√©ponds en Fran√ßais.
    """
    
    try:
        reply = ""
        
        if provider == 'groq':
            if not groq_client:
                 return jsonify({'reply': "‚ö†Ô∏è API Key Groq manquante. Configurez GROQ_API_KEY."}), 200
            
            print("üöÄ Utilisation de Groq (Llama 3)...")
            chat_completion = groq_client.chat.completions.create(
                messages=[{'role': 'system', 'content': context}, {'role': 'user', 'content': user_message}],
                model="llama-3.3-70b-versatile",
            )
            reply = chat_completion.choices[0].message.content
            
        else: # Default to Gemini
            if not GEMINI_API_KEY:
                 return jsonify({'reply': "‚ö†Ô∏è API Key Gemini manquante."}), 200
                 
            print("‚ú® Utilisation de Gemini...")
            model = genai.GenerativeModel('gemini-robotics-er-1.5-preview')
            response = model.generate_content(f"{context}\n\nQuestion Utilisateur : {user_message}")
            reply = response.text

        return jsonify({'reply': reply})
        
    except Exception as e:
        error_msg = str(e)
        print(f"Erreur IA: {error_msg}")
        return jsonify({'reply': f"Erreur technique ({provider}) : {error_msg}"}), 200

if __name__ == '__main__':
    print("\n" + "="*60)
    print("üöÄ API DE PR√âDICTION D'ATTRITION DES EMPLOY√âS")
    print("="*60 + "\n")
    
    if not load_model():
        print("\n‚ö†Ô∏è  L'API va d√©marrer mais les pr√©dictions ne fonctionneront pas.")
    
    print("\nüì° Endpoints disponibles:")
    print("   GET  /health  - V√©rifier le statut de l'API")
    print("   POST /predict - Pr√©dire l'attrition")
    print("   POST /chat    - Discuter avec l'Assistant RH")
    
    print("\nüåê L'API d√©marre sur http://localhost:5000")
    print("="*60 + "\n")
    
    app.run(debug=True, host='0.0.0.0', port=5000)
